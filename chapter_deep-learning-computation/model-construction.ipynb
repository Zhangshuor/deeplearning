{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层和块\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。\n",
    "在这里，整个模型只有一个输出。\n",
    "注意，单个神经网络\n",
    "（1）接受一些输入；\n",
    "（2）生成相应的标量输出；\n",
    "（3）具有一组相关 *参数*（parameters），更新这些参数可以优化某目标函数。\n",
    "\n",
    "然后，当考虑具有多个输出的网络时，\n",
    "我们利用矢量化算法来描述整层神经元。\n",
    "像单个神经元一样，层（1）接受一组输入，\n",
    "（2）生成相应的输出，\n",
    "（3）由一组可调整参数描述。\n",
    "当我们使用softmax回归时，一个单层本身就是模型。\n",
    "然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。\n",
    "\n",
    "对于多层感知机而言，整个模型及其组成层都是这种架构。\n",
    "整个模型接受原始输入（特征），生成输出（预测），\n",
    "并包含一些参数（所有组成层的参数集合）。\n",
    "同样，每个单独的层接收输入（由前一层提供），\n",
    "生成输出（到下一层的输入），并且具有一组可调参数，\n",
    "这些参数根据从下一层反向传播的信号进行更新。\n",
    "\n",
    "事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。\n",
    "例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层，\n",
    "这些层是由*层组*（groups of layers）的重复模式组成。\n",
    "这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛\n",
    "的识别和检测任务 :cite:`He.Zhang.Ren.ea.2016`。\n",
    "目前ResNet架构仍然是许多视觉任务的首选架构。\n",
    "在其他的领域，如自然语言处理和语音，\n",
    "层组以各种重复模式排列的类似架构现在也是普遍存在。\n",
    "\n",
    "为了实现这些复杂的网络，我们引入了神经网络*块*的概念。\n",
    "*块*（block）可以描述单个层、由多个层组成的组件或整个模型本身。\n",
    "使用块进行抽象的一个好处是可以将一些块组合成更大的组件，\n",
    "这一过程通常是递归的，如 :numref:`fig_blocks`所示。\n",
    "通过定义代码来按需生成任意复杂度的块，\n",
    "我们可以通过简洁的代码实现复杂的神经网络。\n",
    "\n",
    "![多个层被组合成块，形成更大的模型](http://d2l.ai/_images/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "从编程的角度来看，块由*类*（class）表示。\n",
    "它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，\n",
    "并且必须存储任何必需的参数。\n",
    "注意，有些块不需要任何参数。\n",
    "最后，为了计算梯度，块必须具有反向传播函数。\n",
    "在定义我们自己的块时，由于自动微分（在 :numref:`sec_autograd` 中引入）\n",
    "提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。\n",
    "\n",
    "在构造自定义块之前，(**我们先回顾一下多层感知机**)\n",
    "（ :numref:`sec_mlp_concise` ）的代码。\n",
    "下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，\n",
    "然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0441,  0.1836,  0.0693,  0.1147, -0.0625, -0.0748,  0.0575,  0.0870,\n",
       "         -0.1349,  0.1336],\n",
       "        [ 0.0240,  0.1013,  0.1543,  0.1009, -0.0523, -0.0031,  0.0363,  0.0658,\n",
       "          0.0404,  0.0626]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F#定义了一些函数，一些没有包括参数的一些函数\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))#nn.Sequential定义了一种特殊的Module\n",
    "\n",
    "X = torch.rand(2,20)#一个批量，给两个20维的向量\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义一个块MLP，实现和上面一样的模型块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    #网络里面要的层，都在这里\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    #前向函数  \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))#把输入x放入hidden中,调用relu，放入输出\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0536,  0.0803, -0.0045,  0.0013, -0.1588,  0.1755, -0.0112, -0.0641,\n",
       "          0.1367,  0.1616],\n",
       "        [-0.0869,  0.0872, -0.0020, -0.0030, -0.1718,  0.1017,  0.0556, -0.1058,\n",
       "          0.0182,  0.2378]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1360,  0.0181, -0.0122, -0.2132,  0.1704,  0.0068,  0.0554, -0.0363,\n",
       "          0.0641,  0.0251],\n",
       "        [-0.0553,  0.1357, -0.0201, -0.1665,  0.1047, -0.2056,  0.1603,  0.0227,\n",
       "          0.0407,  0.0387]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):#传入参数 nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10)\n",
    "        super().__init__()\n",
    "        for block in args:#block按顺序拿到从前到后的层\n",
    "            self._modules[block] = block#把这些层按顺序放进modules这个层里面\n",
    "    \n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():#按顺序\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，`MySequential`的用法与之前为`Sequential`类编写的代码相同\n",
    "（如 :numref:`sec_mlp_concise` 中所述）。\n",
    "\n",
    "## [**在前向传播函数中执行代码**]\n",
    "\n",
    "`Sequential`类使模型构造变得简单，\n",
    "允许我们组合新的架构，而不必定义自己的类。\n",
    "然而，并不是所有的架构都是简单的顺序架构。\n",
    "当需要更强的灵活性时，我们需要定义自己的块。\n",
    "例如，我们可能希望在前向传播函数中执行Python的控制流。\n",
    "此外，我们可能希望执行任意的数学运算，\n",
    "而不是简单地依赖预定义的神经网络层。\n",
    "\n",
    "到目前为止，\n",
    "我们网络中的所有操作都对网络的激活值及网络的参数起作用。\n",
    "然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，\n",
    "我们称之为*常数参数*（constant parameter）。\n",
    "例如，我们需要一个计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，\n",
    "其中$\\mathbf{x}$是输入，\n",
    "$\\mathbf{w}$是参数，\n",
    "$c$是某个在优化过程中没有更新的指定常量。\n",
    "因此我们实现了一个`FixedHiddenMLP`类，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0660, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "    \n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以[**混合搭配各种组合块的方法**]。\n",
    "在下面的例子中，我们以一些想到的方法嵌套块。\n",
    "*疯狂套娃*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1320, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 如果将`MySequential`中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    ">简而言之，_modules的主要优点是： 在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 实现一个块，它以两个块为参数，例如`net1`和`net2`，并返回前向传播中两个网络的串联输出。这也被称为平行块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2576,  0.4984, -0.0259,  0.3884,  0.4823,  0.2298, -0.1054,  0.1379,\n",
       "          0.3945,  0.1236],\n",
       "        [-0.2665,  0.3981, -0.1709,  0.3309,  0.4362, -0.1794, -0.0214,  0.2575,\n",
       "          0.5980, -0.0678],\n",
       "        [ 0.0271,  0.1219, -0.9192,  0.2837,  0.1473, -0.2107,  0.2581,  0.6597,\n",
       "          0.0508,  0.7701],\n",
       "        [-0.1257,  0.4043, -0.5683, -0.1146,  0.0117, -0.0702,  0.2947,  0.2350,\n",
       "         -0.1586,  0.8593]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Paralles(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net1 = nn.Sequential(nn.Linear(20, 10))\n",
    "        self.net2 = nn.Sequential(nn.Linear(20, 10))\n",
    "\n",
    "    def forward(self, X):\n",
    "        one = self.net1(X)\n",
    "        two = self.net2(X)\n",
    "        return torch.cat((one,two))\n",
    "net = Paralles()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.7731e-01,\n",
       "          2.4178e-01,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  1.0308e-01,  ...,  1.1567e-01,\n",
       "          2.7966e-02,  7.3634e-03],\n",
       "        [-8.0771e-01,  1.2782e-02,  7.0961e-04,  ...,  1.6663e-01,\n",
       "         -4.3539e-01,  7.8132e-01],\n",
       "        [-8.1357e-01,  2.2365e-01, -1.7543e-01,  ...,  4.8356e-01,\n",
       "         -2.3926e-01,  6.9038e-01]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 假设你想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "monster(\n",
       "  (block): Sequential(\n",
       "    (0): NestMLP(\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=20, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (linear): Linear(in_features=32, out_features=16, bias=True)\n",
       "    )\n",
       "    (1): Linear(in_features=16, out_features=20, bias=True)\n",
       "    (2): FixedHiddenMLP(\n",
       "      (linear): Linear(in_features=20, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (other): Linear(in_features=16, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class monster(nn.Module):\n",
    "    def __init__(self,module,mutation_nums):\n",
    "        super().__init__()\n",
    "        self.block = module\n",
    "        self.nums = mutation_nums\n",
    "        self.other = nn.Linear(16,20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for i in range(self.nums):\n",
    "            X = self.block(X)\n",
    "            X = self.other(X)\n",
    "        return X\n",
    "\n",
    "X = monster(chimera,3)\n",
    "X  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee52bf41c54e33635a2d012c273a7bdcb57cc3fbaa8ae23dccd0e3301b36ba4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
